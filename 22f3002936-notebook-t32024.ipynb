{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":85062,"databundleVersionId":9578279,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Load the data\ntrain_data = pd.read_csv(\"/kaggle/input/predict-the-success-of-bank-telemarketing/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/predict-the-success-of-bank-telemarketing/test.csv\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Overview\n\nThe dataset structure and key insights based on the `info()` output are as follows:\n\n1. **Total Entries (Rows)**: The dataset contains  **39211 rows**.\n2. **Total Columns**: There are **16 columns** in the dataset.\n3. **Data Types**:\n   - **Integer (`int64`)**: 6 column\n   - **Float (`float64`)**: 0 column\n   - **Object (`object`)**: 10 column\n   - **Boolean (`bool`)**: 0 column\n4. **Missing Values**:\n   - `Column3` has **229 missing values**.\n   - `Column5` has **1467 missing values**.\n   - `Column10` has **10336 missing values**.\n   - `Column15` has **29451 missing values**.\n5. **Memory Usage**: The dataset uses approximately **4.8 MB** of memory.\n\n### Additional Observations\n- Other Columns have no missing values, ensuring their reliability.\n\n### Suggestions for Data Preparation\n- Handle missing values in `Column3`, `Column5`, `Column10` and `Column15` before proceeding with training.\n- Verify the data types for correctness, especially for `object` and `category` columns.\n- Explore the categorical distributions in `object` columns to understand its significance.","metadata":{"editable":false}},{"cell_type":"code","source":"train_data.head()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initial Data Preview (`head()`)\n\nHere is a quick summary of the first few rows of the dataset:\n\n1. The dataset contains columns like `last contact date`, `age`, `job`, `marital` etc., which represent the personal life of person with his/her banking data and campaigns for bank deposits.\n2. Data:\n   - `Column1` seems to be last day on which contact made to the person by the bank.\n   - `Column2 - Column5` contains personal information with some missing values (`NaN`).\n   - `Column6 - Column10` appears to hold Banking information such as loan status, contact type and defaulter status.\n   - `Column11 - Column15` appears to hold campaigns data for the person by the bank.\n   - `Column16` reprsents the campaign suceess information as `yes` or `no`.\n3. Observed patterns:\n   - `Column16` has only `yes` and `no` values, indicating it is a `Binary Classification` problem.\n   \n**Action Points**:\n- Investigate the data types further for accuracy.\n","metadata":{"editable":false}},{"cell_type":"code","source":"train_data.describe()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Statistical Summary (`describe()`)\n\n### Key Observations:\n1. **Summary of Numerical Columns**:\n   - `Column2`:\n     - Minimum: 18\n     - Maximum: 95\n     - Mean: 40 \n   - `Column7`:\n     - Mean: 5441.781719, Standard Deviation: 16365.292065.\n     - Minimum and Maximum values: -8019 and 102127.\n     - Some outliers may exist, as the range is wide.\n   - `Column11`:\n     - Mean: 439.062789, Standard Deviation: 769.096291.\n     - Minimum and Maximum values: 0 and 4918.\n     - Some outliers may exist, as the range is wide.\n   - `Column12`:\n     - Mean: 5.108770, Standard Deviation: 9.890153.\n     - Minimum and Maximum values: 1 and 63.\n     - Some outliers may exist, as the range is wide.\n   - `Column13`:\n     - Mean: 72.256051, Standard Deviation: 160.942593.\n     - Minimum and Maximum values: -1 and 871.\n     - Some outliers may exist, as the range is wide.\n   - `Column14`:\n     - Mean: 11.826171, Standard Deviation: 44.140259.\n     - Minimum and Maximum values: 0 and 275.\n     - Some outliers may exist, as the range is wide.\n\n2. **Notable Trends**:\n   - `Column7, Column11-Column14` has a significant standard deviation, indicating high variability.\n   - Skewness or outliers should be checked in `Column2` based on its range.\n\n3. **Missing Data**:\n   - `describe()` excludes null values in calculations. Cross-check against `info()` for accurate missing value handling.\n\n**Action Points**:\n- Perform further analysis on `Column7, Column11-Column14` for skewness or outliers.\n- Normalize/Scale the data in columns with high variability for modeling purposes.\n","metadata":{"editable":false}},{"cell_type":"code","source":"test_data.info()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Overview\n\nThe dataset structure and key insights based on the `info()` output are as follows:\n\n1. **Total Entries (Rows)**: The dataset contains  **10000 rows**.\n2. **Total Columns**: There are **15 columns** in the dataset.\n3. **Data Types**:\n   - **Integer (`int64`)**: 6 column\n   - **Float (`float64`)**: 0 column\n   - **Object (`object`)**: 10 column\n   - **Boolean (`bool`)**: 0 column\n4. **Missing Values**:\n   - `Column3` has **59 missing values**.\n   - `Column5` has **390 missing values**.\n   - `Column10` has **2684 missing values**.\n   - `Column15` has **7508 missing values**.\n5. **Memory Usage**: The dataset uses approximately **1.1 MB** of memory.\n\n### Additional Observations\n- Other Columns have no missing values, ensuring their reliability.\n\n### Suggestions for Data Preparation\n- Handle missing values in `Column3`, `Column5`, `Column10` and `Column15` before proceeding with predicting.\n- Verify the data types for correctness, especially for `object` and `category` columns.\n- Explore the categorical distributions in `object` columns to understand its significance.","metadata":{"editable":false}},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Initial Data Preview (`head()`)\n\nHere is a quick summary of the first few rows of the dataset:\n\n1. The dataset contains columns like `last contact date`, `age`, `job`, `marital` etc., which represent the personal life of person with his/her banking data and campaigns for bank deposits.\n2. Data:\n   - `Column1` seems to be last day on which contact made to the person by the bank.\n   - `Column2 - Column5` contains personal information with some missing values (`NaN`).\n   - `Column6 - Column10` appears to hold Banking information such as loan status, contact type and defaulter status.\n   - `Column11 - Column15` appears to hold campaigns data for the person by the bank.\n   \n**Action Points**:\n- Investigate the data types further for accuracy.\n","metadata":{"editable":false}},{"cell_type":"code","source":"test_data.describe()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Statistical Summary (`describe()`)\n\n### Key Observations:\n\n1. **Notable Trends**:\n   - `Column7, Column11-Column14` has a significant standard deviation, indicating high variability.\n   - Skewness or outliers should be checked in `Column2` based on its range.\n\n2. **Missing Data**:\n   - `describe()` excludes null values in calculations. Cross-check against `info()` for accurate missing value handling.\n\n**Action Points**:\n- Perform further analysis on `Column7, Column11-Column14` for skewness or outliers.\n- Normalize/Scale the data in columns with high variability for modeling purposes.\n","metadata":{"editable":false}},{"cell_type":"code","source":"# Checking duplicates\ntrain_data.loc[train_data.duplicated()]","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Duplicate Detection\n\nWe do not found any duplicate rows in our dataset which is crucial for removing redundency and in training the model.","metadata":{"editable":false}},{"cell_type":"code","source":"# Checking Age Distribution\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize=(11, 4))\n\n# KDE plot\nsns.kdeplot(data=train_data, x='age', ax=ax[0], fill = True)\nax[0].set_ylabel('')\n\n# Boxplot\nsns.boxplot(data=train_data, x='age', ax=ax[1])\n\n# Set main title for all subplots\nplt.suptitle('Age Distribution')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking job Distribution\n\n# Get the order of categories\norder = train_data['job'].value_counts().index\n\n# Set figure size\nplt.figure(figsize=(11, 4))\n\n# Create countplot\nsns.countplot(data=train_data, y='job', order=order).set_ylabel('')\n\n# Set main title for all subplots\nplt.suptitle('Job Distribution')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Marital Status Distribution\n\n# Get the order of categories\norder = train_data['marital'].value_counts().index\n\n# Set figure size\nplt.figure(figsize=(11, 4))\n\n# Create countplot\nsns.countplot(data=train_data, y='marital', order=order).set_ylabel('')\n\n# Set main title for all subplots\nplt.suptitle('Marital Status Distribution')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Educational Qualification Distribution\n\n# Get the order of categories\norder = train_data['education'].value_counts().index\n\n# Set figure size\nplt.figure(figsize=(11, 4))\n\n# Create countplot\nsns.countplot(data=train_data, y='education', order=order).set_ylabel('')\n\n# Set main title for all subplots\nplt.suptitle('Education Distribution')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Credit default Distribution\n\n# Get the order of categories\norder = train_data['default'].value_counts().index\n\n# Set figure size\nplt.figure(figsize=(11, 4))\n\n# Create countplot\nsns.countplot(data=train_data, y='default', order=order).set_ylabel('')\n\n# Set main title for all subplots\nplt.suptitle('Credits in default Distribution')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Balance Distribution\n\nfig, ax = plt.subplots(1, 2, figsize=(11, 4))\n\n# KDE plot\nsns.kdeplot(data=train_data, x='balance', ax=ax[0], fill = True)\nax[0].set_ylabel('')\n\n# Boxplot\nsns.boxplot(data=train_data, x='balance', ax=ax[1])\n\n# Set main title for all subplots\nplt.suptitle('Balance Distribution')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Housing Loan Status Distribution\n\n# Get the order of categories\norder = train_data['housing'].value_counts().index\n\n# Set figure size\nplt.figure(figsize=(11, 4))\n\n# Create countplot\nsns.countplot(data=train_data, y='housing', order=order).set_ylabel('')\n\n# Set main title for all subplots\nplt.suptitle('Housing loan Distribution')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Loan Status Distribution\n\n# Get the order of categories\norder = train_data['loan'].value_counts().index\n\n# Set figure size\nplt.figure(figsize=(11, 4))\n\n# Create countplot\nsns.countplot(data=train_data, y='loan', order=order).set_ylabel('')\n\n# Set main title for all subplots\nplt.suptitle('Loan Distribution')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Contact Type Distribution\n\n# Get the order of categories\norder = train_data['contact'].value_counts().index\n\n# Set figure size\nplt.figure(figsize=(11, 4))\n\n# Create countplot\nsns.countplot(data=train_data, y='contact', order=order).set_ylabel('')\n\n# Set main title for all subplots\nplt.suptitle('Contact type Distribution')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Contact Duration Distribution\n\nfig, ax = plt.subplots(1, 2, figsize=(11, 4))\n\n# KDE plot\nsns.kdeplot(data=train_data, x='duration', ax=ax[0], fill = True)\nax[0].set_ylabel('')\n\n# Boxplot\nsns.boxplot(data=train_data, x='duration', ax=ax[1])\n\n# Set main title for all subplots\nplt.suptitle('Last contact duration Distribution(in sec)')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Number of contacts during campaign Distribution\n\nfig, ax = plt.subplots(1, 2, figsize=(11, 4))\n\n# KDE plot\nsns.kdeplot(data=train_data, x='campaign', ax=ax[0], fill = True)\nax[0].set_ylabel('')\n\n# Boxplot\nsns.boxplot(data=train_data, x='campaign', ax=ax[1])\n\n# Set main title for all subplots\nplt.suptitle('Number of contacts during campaign')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking no. of days from previous contact day Distribution\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 4))\n\n# KDE plot\nsns.kdeplot(data=train_data, x='pdays', ax=ax[0], fill = True)\nax[0].set_ylabel('')\n\n# Boxplot\nsns.boxplot(data=train_data, x='pdays', ax=ax[1])\n\n# Set main title for all subplots\nplt.suptitle('Number of of days from previous Contact Day Distribution')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking campaign Distribution\n\nfig, ax = plt.subplots(1, 2, figsize=(11, 4))\n\n# KDE plot\nsns.kdeplot(data=train_data, x='previous', ax=ax[0], fill = True)\nax[0].set_ylabel('')\n\n# Boxplot\nsns.boxplot(data=train_data, x='previous', ax=ax[1])\n\n# Set main title for all subplots\nplt.suptitle('Number of contacts before campaign')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking outcome Distribution\n\n# Get the order of categories\norder = train_data['poutcome'].value_counts().index\n\n# Set figure size\nplt.figure(figsize=(11, 4))\n\n# Create countplot\nsns.countplot(data=train_data, y='poutcome', order=order).set_ylabel('')\n\n# Set main title for all subplots\nplt.suptitle('Outcome of the previous marketing campaign')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Target Distribution\n\n# Get the order of categories\norder = train_data['target'].value_counts().index\n\n# Set figure size\nplt.figure(figsize=(11, 4))\n\n# Create countplot\nsns.countplot(data=train_data, y='target', order=order).set_ylabel('')\n\n# Set main title for all subplots\nplt.suptitle('Target Distribution')\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Count Plot of the Target Variable\n\nThe count plot for the target variable **`target`** is shown above:\n\n### Observations:\n1. The dataset is imbalanced:\n   - **\"Yes\"**: approx. 15% of the total records.\n   - **\"No\"**: approx. 85% of the total records.\n2. Imbalanced classes could lead to biased predictions, favoring the majority class.\n\n### Actions Taken:\n- Instead of oversampling or undersampling the data, we addressed the imbalance by using the scale_pos_weight parameter in the training process of the model.\n- `scale_pos_weight` = (`Number of Negative Samples`)/(`Number of Positive Samples`) which is in the training data is `5.7291916938390255`. ","metadata":{"editable":false}},{"cell_type":"code","source":"# Checking correlation\n\n# Select numerical columns\nnumerical_cols = train_data.select_dtypes(include=['float64', 'int64']).columns\n\n# Calculate correlation matrix\ncorr = train_data[numerical_cols].corr()\n\n# Mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap='coolwarm', vmax=1.0, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot = True)\n\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation Analysis\n\n### Key Insights:\n1. **Overall Relationships**:\n   - The correlation matrix reveals weak, and negligible relationships between variables.\n   - Variables with weak positive/negative correlation may not be closely related.\n\n2. **Notable Observations**:\n   - Strong correlations between features may indicate redundancy.\n   - Negative correlations might suggest inverse dependencies, useful for predictive modeling.\n\n### Visualization:\n- The correlation heatmap highlights relationships:\n  - Darker colors ndicate stronger positive/negative correlations.\n  - Lighter colors indicate weaker relationships.\n\n### Recommendations:\n1. **Feature Selection**:\n   - Focus on weakly correlated variables for exploratory analysis.\n\n2. **Further Exploration**:\n   - Remove any outliers and check again for correlation analysis.\n","metadata":{"editable":false}},{"cell_type":"code","source":"from scipy.stats import zscore\n\n# Detecting outliers\n\n# Selecting only numeric columns from df\nnumerical_df = train_data.select_dtypes(include=[np.number])\n\n# Calculating z-scores\nz_scores = numerical_df.apply(zscore)\n\n# Get rows where any column has an absolute z-score > 3\noutlier_rows = train_data[(np.abs(z_scores) > 3).any(axis=1)]\n\n# Print outlier rows\noutlier_rows","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Outlier Detection and Removal\n\nOutliers are extreme data points that deviate significantly from the majority of the dataset. They can:\n- Skew statistical summaries such as mean and standard deviation.\n- Affect model performance by introducing noise.\n\n### Methods Used for Outlier Detection\n1. **Visualization Techniques**:\n   - **Box Plot**: Highlighted potential outliers beyond the whiskers.\n   - **Scatter Plot**: Revealed extreme values in relationships between variables.\n2. **Statistical Methods**:\n   - **Z-Score**: Data points with a Z-score > 3 or < -3 were considered outliers.\n   - **Interquartile Range (IQR)**:\n     - Formula: IQR = Q3 - Q1\n     - Lower Bound: Q1 - 1.5 * IQR\n     - Upper Bound: Q3 + 1.5 * IQR\n\n\n### Outliers Detected\n\nWe have found out that 3222 rows in our training dataset are outliers which can create potential noise in our model training leading to overfitting of the model as the outliers consists of approximately 10% of the total data rows in the train dataset.","metadata":{"editable":false}},{"cell_type":"code","source":"# removing outliers\n\ntrain_data = train_data.drop(outlier_rows.index)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define features and target\nX_train = train_data.drop(['target'], axis=1)\ny_train = train_data['target']","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Encoding target column\nle_target = LabelEncoder()\ny_train = le_target.fit_transform(y_train)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_data = pd.concat([X_train, test_data], axis=0, ignore_index=True)\n\nfrom sklearn.impute import SimpleImputer\n\n# Impute 'job' with most frequent value\nimputer = SimpleImputer(strategy='most_frequent')\nall_data['job'] = imputer.fit_transform(all_data[['job']]).ravel()\n\n# Impute 'education', 'contact', and 'poutcome' with 'unknown'\nimputer = SimpleImputer(strategy='constant', fill_value='unknown')\nfor column in ['education', 'contact', 'poutcome']:\n    all_data[column] = imputer.fit_transform(all_data[[column]]).ravel()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature engineering\ndef feature_engineering(df):\n    # Convert date to datetime\n    df['last contact date'] = pd.to_datetime(df['last contact date'])\n    \n    # Extract date features\n    df['contact_month'] = df['last contact date'].dt.month\n    df['contact_day'] = df['last contact date'].dt.day\n    df['contact_dayofweek'] = df['last contact date'].dt.dayofweek\n    \n    # Create age groups\n    df['age_group'] = pd.cut(df['age'], bins=[0, 20, 30, 40, 50, 60, 100], labels=['0-20', '21-30', '31-40', '41-50', '51-60', '60+'])\n    \n    # Create balance groups\n    df['balance_group'] = pd.qcut(df['balance'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n    \n    # Drop original date column\n    df = df.drop('last contact date', axis=1)\n    \n    return df\n\nall_data = feature_engineering(all_data)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preprocess the data\n\n# Encoding categorical columns\nle = LabelEncoder()\ncategorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome', 'age_group', 'balance_group']\n\nfor col in categorical_columns:\n    all_data[col] = le.fit_transform(all_data[col].astype(str))","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Removing 'pdays' column\n\nall_data.drop(['pdays'], axis=1, inplace=True)\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale numerical features\nscaler = StandardScaler()\nnumerical_columns = ['age', 'balance', 'duration', 'campaign', 'previous']\nall_data[numerical_columns] = scaler.fit_transform(all_data[numerical_columns])","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# split the train and test data after feature engineering\n\nX_train = all_data[:len(X_train)]\nX_test = all_data[len(X_train):]","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom skopt import BayesSearchCV\n\n# Define hyperparameter search space\nparam_grid = {\n    'n_estimators': (100, 500),  # Range for number of trees\n    'max_depth': (3, 10),  # Tree depth\n    'learning_rate': (0.01, 0.1, 'log-uniform'),  # Log scale for finer tuning\n    'min_child_weight': (1, 10),  # Minimum sum of weights for split\n    'subsample': (0.6, 1.0),  # Fraction of samples\n    'colsample_bytree': (0.6, 1.0),  # Fraction of features\n    'gamma': (0, 0.5),  # Minimum loss reduction\n    'reg_alpha': (0, 1),  # L1 regularization\n    'reg_lambda': (0.5, 2),  # L2 regularization\n    'scale_pos_weight': (1, 10)  # Class imbalance adjustment\n}\n\n# Initialize XGBoost model\nxgb = XGBClassifier(\n    random_state=42,\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\n\n# Define scoring metric\nscorer = make_scorer(f1_score, average='macro')\n\n# Set up Bayesian Search\nbayes_search = BayesSearchCV(\n    estimator=xgb,\n    search_spaces=param_grid,\n    scoring=scorer,\n    n_iter=50,  # Number of iterations\n    cv=3,  # 3-fold cross-validation\n    n_jobs=-1,\n    random_state=42,\n    verbose=2\n)\n\n# Fit Bayesian optimizer\nbayes_search.fit(X_train, y_train)\n\n# Best parameters and model\nbest_params = bayes_search.best_params_\nprint(\"Best Parameters:\", best_params)\n\nbest_model = bayes_search.best_estimator_","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train Test Split for creating and getting insight from model prediction with confusion matrix\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_subset, X_eval, y_train_subset, y_eval = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\n# Predict probabilities\ny_prob = best_model.predict_proba(X_eval)[:, 1]\n\n# Compute precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_eval, y_prob)\n\n# Identify the best threshold for maximizing F1-Score\nf1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\noptimal_idx = f1_scores.argmax()\noptimal_threshold = thresholds[optimal_idx]\nprint(\"Optimal Threshold for Precision-Recall Balance:\", optimal_threshold)\n\n# Apply the threshold\ny_pred_adjusted = (y_prob >= optimal_threshold).astype(int)\n\n# Evaluate\nfrom sklearn.metrics import classification_report\nprint(\"Adjusted Classification Report:\\n\", classification_report(y_eval, y_pred_adjusted))\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_eval, y_pred_adjusted)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Confusion Matrix\n\n### Insights\n\n1. From the confusion matrix we can see that the approximately 18% of True(Positive - 1) values are predicted as False(Negative - 0).\n2. And the approximately 2.5% of the False(Negative - 0) values are predicted as True(Positive - 1).\n\n**Note:** From these results we can say that our model has an accuracy of about approximately 90%.","metadata":{"editable":false}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Calculate evaluation metrics\naccuracy = accuracy_score(y_eval, y_pred_adjusted)\nprecision = precision_score(y_eval, y_pred_adjusted)\nrecall = recall_score(y_eval, y_pred_adjusted)\nf1 = f1_score(y_eval, y_pred_adjusted)\n\n# Print the metrics\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1-Score: {f1:.4f}\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on test set\ny_pred_prob = best_model.predict_proba(X_test)[:, 1]\n\ny_pred = (y_pred_prob >= optimal_threshold).astype(int)","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the submission file\nsubmission = pd.DataFrame({'id': range(len(y_pred)), 'target': le_target.inverse_transform(y_pred)})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating Submission File (`submission.csv`)\n\n1. **Inversing the `target - column`** from `1` and `0` to `yes` and `no` respectively.\n2. **Binding target with `id - column`** 0 - 9999 (`10000 rows`).","metadata":{"editable":false}}]}